{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, GCNConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# To compute distances\n",
    "# from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the GCN for regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all we have to define the dataset\n",
    "# Prepare dataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataset = TUDataset(root='/tmp/MUTAG', name='MUTAG', use_node_attr=True)\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:10]\n",
    "test_dataset = dataset[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloader to take as input graph pairs\n",
    "from torch_geometric.data import Data\n",
    "class PairData(Data):\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        if key == 'edge_index_1':\n",
    "            return self.x_1.size(0)\n",
    "        if key == 'edge_index_2':\n",
    "            return self.x_2.size(0)\n",
    "        return super().__inc__(key, value, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct all pairs of graphs using PairData object\n",
    "# Distance for now is initialized at random in a value between (-1, 1) like a cosine distance would\n",
    "import random\n",
    "train_data_list = []\n",
    "for ind1, graph1 in enumerate(train_dataset):\n",
    "    for ind2, graph2 in enumerate(train_dataset[ind1+1:]):\n",
    "        # ind2 += (ind1 + 1)\n",
    "        train_data_list.append(PairData(x_1=graph1.x, edge_index_1=graph1.edge_index,\n",
    "                            x_2=graph2.x, edge_index_2=graph2.edge_index,\n",
    "                            distance = random.uniform(-1,1)))   \n",
    "\n",
    "test_data_list = []\n",
    "for ind1, graph1 in enumerate(test_dataset):\n",
    "    for ind2, graph2 in enumerate(test_dataset[ind1+1:]):\n",
    "        # ind2 += (ind1 + 1)\n",
    "        test_data_list.append(PairData(x_1=graph1.x, edge_index_1=graph1.edge_index,\n",
    "                            x_2=graph2.x, edge_index_2=graph2.edge_index,\n",
    "                            distance = random.uniform(-1,1)))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_list) #binom(n,2) = n(n-1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_pairs(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Takes as input a pair of graphs which are both fed through 3 convolutional layers each followed by an activation function\n",
    "    3 graph convolutional layers (Welling) that share parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features, hidden_channels, output_embeddings):\n",
    "        super(GCN_pairs, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(input_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, output_embeddings)\n",
    "\n",
    "    def forward(self, x1, edge_index1, batch1, x2, edge_index2, batch2): # Need a way to extract these from dataloader\n",
    "        # 1. Obtain node embeddings for graph 1\n",
    "        x1 = self.conv1(x1, edge_index1)\n",
    "        x1 = x1.relu()\n",
    "        x1 = self.conv2(x1, edge_index1)\n",
    "        x1 = x1.relu()\n",
    "        x1 = self.conv3(x1, edge_index1)\n",
    "        # 2. Readout layer\n",
    "        x1 = global_mean_pool(x1, batch1)  # [batch_size, hidden_channels]\n",
    "        # 3. Apply a final linear transformation on the aggregated embedding\n",
    "        x1 = torch.nn.functional.dropout(x1, p=0.5, training=self.training)\n",
    "        x1 = self.lin(x1)\n",
    "\n",
    "        # 1. Obtain node embeddings for graph 2\n",
    "        x2 = self.conv1(x2, edge_index2)\n",
    "        x2 = x2.relu()\n",
    "        x2 = self.conv2(x2, edge_index2)\n",
    "        x2 = x2.relu()\n",
    "        x2 = self.conv3(x2, edge_index2)\n",
    "        # 2. Readout layer\n",
    "        x2 = global_mean_pool(x2, batch2)  # [batch_size, hidden_channels]\n",
    "        # 3. Apply a final linear transformation on the aggregated embedding\n",
    "        x2 = torch.nn.functional.dropout(x2, p=0.5, training=self.training)\n",
    "        x2 = self.lin(x2)\n",
    "\n",
    "        #print(f\"x1 has shape: {x1.shape} and is:\\n\\t{x1}\")\n",
    "        #print(f\"x2 has shape: {x2.shape} and is:\\n\\t{x2}\")\n",
    "        return (x1 - x2).pow(2).sum(1) # This is the euclidean distance between the two returns [dist of pair1, dist of pair2, ..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_pairs(\n",
      "  (conv1): GCNConv(7, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=300, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GCN_pairs(input_features = dataset.num_node_features, hidden_channels=64, output_embeddings=300).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "PairDataBatch(x_1=[223, 7], x_1_batch=[223], x_1_ptr=[17], edge_index_1=[2, 482], x_2=[245, 7], x_2_batch=[245], x_2_ptr=[17], edge_index_2=[2, 528], distance=[16])\n",
      "tensor([ 0.1234, -0.3736,  0.7904,  0.6997, -0.4735, -0.0471, -0.6616,  0.5039,\n",
      "         0.3501,  0.7905,  0.0886, -0.0820, -0.8334, -0.1336,  0.9230, -0.4689])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "PairDataBatch(x_1=[243, 7], x_1_batch=[243], x_1_ptr=[17], edge_index_1=[2, 508], x_2=[248, 7], x_2_batch=[248], x_2_ptr=[17], edge_index_2=[2, 544], distance=[16])\n",
      "tensor([-0.9048,  0.8163, -0.4775,  0.4399,  0.7116,  0.1004,  0.1144,  0.8433,\n",
      "         0.5529, -0.7445,  0.0731,  0.3546, -0.5416, -0.6670, -0.6884, -0.4920])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 13\n",
      "PairDataBatch(x_1=[220, 7], x_1_batch=[220], x_1_ptr=[14], edge_index_1=[2, 490], x_2=[180, 7], x_2_batch=[180], x_2_ptr=[14], edge_index_2=[2, 382], distance=[13])\n",
      "tensor([ 0.1419, -0.9760, -0.4920, -0.6443,  0.2464, -0.4198,  0.1708,  0.7959,\n",
      "        -0.1263,  0.8997,  0.0887, -0.6005,  0.6182])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data_list, batch_size=16, follow_batch=['x_1', 'x_2'])\n",
    "test_loader = DataLoader(test_data_list, batch_size=16, follow_batch=['x_1', 'x_2'])\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    data = data.to(device)\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print(data.distance)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x_1, data.edge_index_1, data.x_1_batch, \n",
    "                    data.x_2, data.edge_index_2, data.x_2_batch)  # Perform a single forward pass.\n",
    "        # print(f\"out is:\\n\\t{out}\\nwhile dist is {data.distance}\")\n",
    "        loss = criterion(out, data.distance)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            out = model(data.x_1, data.edge_index_1, data.x_1_batch, \n",
    "                        data.x_2, data.edge_index_2, data.x_2_batch)  # Perform a single forward pass.\n",
    "            loss = criterion(out, data.distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 000,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 010, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 010,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 020, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 020,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 030, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 030,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 040, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 040,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 050, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 050,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 060, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 060,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 070, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 070,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 080, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 080,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 090, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 090,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 100, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 100,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 110, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 110,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 120, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 120,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 130, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 130,  train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 140, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f\n",
      "Epoch: 140,  train_acc:.4f, Test Acc: test_acc:.4f\n"
     ]
    }
   ],
   "source": [
    "model = GCN_pairs(input_features=dataset.num_node_features, hidden_channels=64, output_embeddings=300)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_loader = DataLoader(train_data_list, batch_size=16, follow_batch=['x_1', 'x_2'])\n",
    "\n",
    "\n",
    "epochs=150\n",
    "for epoch in range(epochs):\n",
    "    train()\n",
    "    test()\n",
    "    #train_acc = test(train_loader)\n",
    "    #test_acc = test(test_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: train_acc:.4f, Test Acc: test_acc:.4f')\n",
    "        print(f'Epoch: {epoch:03d},  train_acc:.4f, Test Acc: test_acc:.4f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "- Add the test during the training procedure and print some metric like MSE at every iteration in training and test set for example.\n",
    "- Do it with the actual homomorphism counts distances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TUW_ML_Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
